{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "476ce6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Date column dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "#  === Step 2: Drop the Date column (not useful for numeric models)\n",
    "df = df.drop(columns=[\"Date\"], errors=\"ignore\")\n",
    "print(\"✅ Date column dropped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "768afbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features and target variable defined.\n"
     ]
    }
   ],
   "source": [
    "#  === Step 3: Define features and target variable\n",
    "X = df.drop(columns=[\"Close\"])  # Features: everything except Close price\n",
    "y = df[\"Close\"]                 # Target: the closing price\n",
    "print(\"✅ Features and target variable defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "c134dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split into training and testing sets.\n",
      "Training set size: 2170 samples\n",
      "Testing set size: 543 samples\n"
     ]
    }
   ],
   "source": [
    "#  === Step 4: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset split information\n",
    "print(\"✅ Data successfully split into training and testing sets.\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "fdf7a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Linear Regression trained successfully.\n",
      "✅ Random Forest Regressor trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# === Step 5: Train Models ===\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"✅ {name} trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "7222ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions are ready!\n",
      "\n",
      "Linear Regression predictions: [  386.549011    4017.268555    7448.307617    1133.25\n",
      " 43099.699219     243.593994     386.354004   17706.900391\n",
      " 55907.19921899 41821.261719     777.94397     2548.290039\n",
      "  4073.26001      281.653992   11657.200195    9771.489258\n",
      "  1205.01001    50429.859375   57229.828125     447.610992\n",
      "  8745.894531    8820.52246099 11600.099609   57401.097656\n",
      "   447.976013   10948.990234     455.67099     5903.439941\n",
      "  1929.819946    8550.760742    7068.47998    65466.839844\n",
      "  9525.750977    6423.759766     456.078003     416.437988\n",
      " 41500.875        973.497009     580.182007     254.320007\n",
      "   374.785004     758.700012    6371.299805    7037.580078\n",
      "   217.110992    8206.145508   61888.832031   10400.915039\n",
      "  9800.636719     422.744995     657.070984    3894.130859\n",
      "  9264.813477    9412.612305    1100.22998      228.572998\n",
      "   375.490997    1187.810059   35615.871094    9344.365234\n",
      "   376.522003    1211.670044     233.542999    4087.659912\n",
      "  7884.90918      271.912994    6582.359863     320.04599\n",
      "   296.378998   16099.799805    6734.803711    7973.20752\n",
      " 56280.425781   17150.62304699 11878.111328   44963.074219\n",
      "  7569.629883   35510.28906299   328.015015   56473.03124999\n",
      "   432.519012   57424.00781299  8805.77832     1167.540039\n",
      "  1154.72998     8163.419922   46306.445313   45201.457031\n",
      "   389.54599    51679.79687499 40797.60937499 32067.642578\n",
      "  6681.062988    1166.719971   16624.599609   55033.11718799\n",
      "   236.929001     377.321014     317.842987     338.321014\n",
      "   279.584991   10915.68554699  7218.371094   12899.200195\n",
      " 11555.363281     441.389008    3486.950195   45585.03125\n",
      "  5831.16748      921.789001     284.894012    5590.689941\n",
      "   325.431        641.072021     709.848022     638.645996\n",
      "   247.526001    3381.280029   44883.910156    9151.392578\n",
      "   247.272003     416.321991    5922.042969   56396.51562499\n",
      "  6461.009766   11523.579102    3954.118164    8192.150391\n",
      "  9055.526367   52246.523438    3655.006836    6198.77831999\n",
      "   654.096985     368.369995    8879.620117     310.867004\n",
      "   452.727997     423.561005    6517.310059    3854.35791\n",
      "  6242.193848    5324.551758    8208.995117    1124.780029\n",
      " 32289.378906     373.446991    1021.75         214.860992\n",
      "  2372.560059    8866.           751.616028     361.188995\n",
      " 10518.174805   10242.347656    6529.169922     323.04599\n",
      "  6359.490234    9081.761719    6305.799805   55137.31249999\n",
      "  6955.27002      420.903992    6640.515137   46091.39062499\n",
      " 56099.51953099  7217.427246    9341.705078    1723.349976\n",
      "   453.230011    7047.916992   32186.277344     715.533997\n",
      " 10185.5         6486.390137   33155.847656    6438.644531\n",
      "   380.554993    3631.040039   10399.668945    6322.689941\n",
      "   311.084015    3985.080811   17776.699219    7276.802734\n",
      "  7633.759766   40008.421875     243.863007    4257.419922\n",
      "  1080.5        13271.285156    4376.629883   39097.859375\n",
      "  1061.349976     382.556      58119.578125    5768.289551\n",
      "  7165.700195   23477.294922    6453.720215    7624.919922\n",
      "   711.619019     281.881989     245.595001     907.609985\n",
      "   272.722992    4065.199951    7807.058594     345.304993\n",
      "   347.376007     231.492996   46707.015625    4376.529785\n",
      "  9360.879883    6376.709961   11754.045898    6985.470215\n",
      "  9729.324219     231.212006   48717.28906299 46339.76171895\n",
      "   577.469971    6890.52002     8223.679688    2175.469971\n",
      "  9119.009766     235.427002    7954.12793      326.927002\n",
      "  6853.839844     336.752991     233.513        400.570007\n",
      "   325.748993   17527.          8163.692383   38436.96875\n",
      "   587.559021    2398.840088    3843.52002     3742.700439\n",
      " 32127.26757799  7152.301758     781.481018   28840.95312499\n",
      "  3252.909912     219.429993    8835.052734    1804.910034\n",
      "  9842.66601599   438.639008    9164.231445     261.550995\n",
      "   289.606995     281.88501     7124.673828     426.765015\n",
      " 63503.45703099 56048.93749999 11790.916992     233.843002\n",
      "  4122.939941     285.217987   10709.652344    5059.817383\n",
      "   239.839996     689.651001     222.925995    8259.992188\n",
      " 11607.400391     229.809998   47706.117188     237.095993\n",
      "   414.065002     632.828003    3923.918701     273.09201\n",
      "  8736.980469   23735.949219    7117.20752      639.890015\n",
      "   226.425003   11225.299805   35698.296875    8693.833008\n",
      "  3620.810791     606.590027     586.752991     454.984985\n",
      "   917.585999   11182.806641   23783.029297     223.832993\n",
      "  8486.993164    8319.472656     281.601013    9551.714844\n",
      "  3599.765869   11759.592773    6786.02002      573.216003\n",
      "  7238.966797    1117.439941    8209.400391    3466.357422\n",
      "  6880.323242   58245.003906   57274.679688   43961.859375\n",
      "  1755.359985   10363.13867199   253.697006    2671.780029\n",
      "  1004.549988     256.335999     235.343994   12254.402344\n",
      "  8723.94043    43177.398438    8269.80957      423.989014\n",
      " 10462.259766   61243.08593799  8245.623047   56804.90234399\n",
      "  8784.494141   40126.429688    9137.993164    1013.380005\n",
      " 47105.51562499  5064.487793    5904.830078     310.737\n",
      " 32110.693359     463.615997     654.35199      380.289001\n",
      "  9194.849609    7290.088379     232.078995     229.781998\n",
      " 33537.17578099  6595.410156     286.188995     342.415009\n",
      "  8228.783203   47345.21875     7881.84668    57569.074219\n",
      " 33897.046875    8094.319824     243.931      54824.11718799\n",
      "  9281.509766     442.401001     391.726013    4151.52002\n",
      " 55361.449219   50050.86718799  6911.089844   39201.945313\n",
      "   222.882004   10360.546875    3576.032471    8723.786133\n",
      "  8897.46875      313.855011     363.183014     421.444\n",
      " 15290.902344     236.462006    7988.560547   35551.957031\n",
      "  2506.370117   42412.433594   64261.992188   14026.599609\n",
      "   358.041992   59057.87890599   364.330994     416.394012\n",
      " 11440.700195    4106.660156   11711.505859   39747.503906\n",
      "  7531.663574    2320.419922    7143.580078   36654.328125\n",
      "   592.103027     711.521973   57005.425781    3654.833496\n",
      " 11680.820313     250.895004   19140.800781     317.239014\n",
      " 39406.941406     225.873993     331.885986     237.115997\n",
      " 54771.578125   11991.233398   11786.299805    4328.410156\n",
      " 11970.478516    6741.75       47093.85156299 10167.268555\n",
      " 35552.515625   60930.835938    9386.788086   10214.37988299\n",
      "   434.334015     705.054016    6568.22998    10141.996094\n",
      " 10725.599609   10442.170898    3678.924561    6429.841797\n",
      " 29001.720703   36824.36328099 10575.533203    7200.174316\n",
      " 10669.58300799 11296.361328     547.465027    4382.879883\n",
      "   423.412994     451.937988    6867.527344     438.714996\n",
      "  2273.429932    6388.439941     383.757996    9303.629883\n",
      "  7557.819824    3690.188232    3836.741211    7252.034668\n",
      "   999.18103     8205.369141     367.572998   12407.332031\n",
      "  9324.717773     437.164001   56942.136719     924.672974\n",
      "  1555.449951    6385.620117    3637.52002      422.483002\n",
      "  9228.325195     773.872009   37154.601563     241.112\n",
      "   455.653015     387.536011     264.195007   13437.88281299\n",
      " 11478.168945    7343.895508     253.828003    7927.714355\n",
      "   276.049011     237.283005    6859.083008    8929.280273\n",
      "  6173.22998     5982.45752     9243.213867    3882.696289\n",
      "   735.382019    1143.810059    6495.839844    7422.652832\n",
      " 10280.351563    4142.526855    8277.009766    7707.770996\n",
      " 11664.847656   19345.121094    1011.799988    3915.714355\n",
      "   352.68399     2608.560059     661.284973    7176.414551\n",
      " 11774.595703     436.571991    6351.799805   56216.18359399\n",
      "   744.593994     244.533997     649.359985   10346.760742\n",
      " 59697.195313    4565.299805    6249.180176     526.232971\n",
      " 43569.003906    9268.76171899   732.034973    3847.175781\n",
      "   327.924011     289.589996   32569.84960899 10701.691406\n",
      " 11358.101563     432.152008   63326.988281     224.951996\n",
      " 40030.976563    7406.52002     7688.077148     651.783997\n",
      "   286.393005    9858.150391   10594.493164     610.892029\n",
      "   639.192993     447.990997   15332.31543    10750.723633\n",
      "  2717.02002    38903.44140599 64995.230469   11601.472656\n",
      "   703.130981   11246.348633   18621.314453   10334.974609\n",
      "  8804.477539     240.348007    6589.620117   11450.84668\n",
      "   228.761002    3378.939941    1267.119995   16477.599609\n",
      " 58313.64453099  6971.091797    7321.988281  ]\n",
      "\n",
      "Random Forest predictions: [  386.6402594   4023.55449215  7458.09250463  1140.47999517\n",
      " 43092.49445333   243.54867847   388.88393779 17721.30851578\n",
      " 56009.51246114 41656.27124996   773.92886228  2555.67560286\n",
      "  4079.56180657   281.27200646 11689.08664055  9765.79316396\n",
      "  1203.96780147 50616.43652361 57263.66261744   449.03266627\n",
      "  8743.02944352  8813.11844705 11660.50218753 57358.432383\n",
      "   448.84794065 10946.29758807   455.13189368  5928.35031729\n",
      "  1974.40147814  8570.80412132  7072.67511225 64873.24628936\n",
      "  9525.75891585  6419.38169438   455.19007497   416.60355727\n",
      " 41355.59277353   972.12091813   580.89558461   254.27222408\n",
      "   375.02703396   761.18766858  6375.83166026  7033.63608889\n",
      "   218.30595313  8207.57839843 61879.68320325 10402.24564456\n",
      "  9793.33881819   422.53052648   657.44456658  3892.25967532\n",
      "  9269.53914036  9420.51110344  1134.7462074    228.57038826\n",
      "   375.33330329  1207.26319452 35630.86492186  9347.36868182\n",
      "   376.31693604  1208.79870359   233.29577339  4093.41607916\n",
      "  7890.941792     270.82125692  6586.55556148   319.85907387\n",
      "   294.18176186 15913.06636725  6732.50602518  7979.75230962\n",
      " 56396.39324223 17512.15957038 11891.20105451 44882.28671884\n",
      "  7574.08495596 35563.44781253   327.82532796 56313.01457044\n",
      "   432.4159958  57440.16960969  8810.22266582  1171.07280392\n",
      "  1119.05600576  8163.55205079 46427.12535189 45027.32472656\n",
      "   390.43700679 51823.77558586 40709.92378922 31993.17148455\n",
      "  6679.93174812  1163.46787959 16684.25642595 54855.19296871\n",
      "   236.78559854   376.79804021   318.4838685    340.01994922\n",
      "   279.18387044 10918.63413083  7210.86589829 12528.79696286\n",
      " 11570.59851548   442.38147689  3473.04888444 45720.51371098\n",
      "  5817.19046878   918.95464608   284.29712601  5586.30551771\n",
      "   325.11031991   637.19316121   709.3861524    639.53963264\n",
      "   247.37238278  3384.06718743 44860.53132818  9149.60197255\n",
      "   246.92709387   416.31913547  5967.44260733 56372.16519549\n",
      "  6461.0046678  11505.73488255  3955.02055207  8194.93871093\n",
      "  9037.03131837 52020.65499999  3654.1425437   6194.16202633\n",
      "   654.76538753   368.96942274  8872.16893564   304.01761425\n",
      "   453.35257729   423.62141765  6516.39343755  3853.64657692\n",
      "  6226.67767574  5337.46500982  8205.65435054  1133.02830818\n",
      " 32329.98671882   373.37328393  1025.24898546   213.84356074\n",
      "  2359.49575429  8867.44596685   753.62543714   357.95366375\n",
      " 10524.84157244 10236.90130869  6531.6954299    322.67604294\n",
      "  6361.51519068  9070.80529283  6305.88924812 54785.66734373\n",
      "  6962.13139167   420.74296609  6641.27963866 46076.72613277\n",
      " 55937.71875012  7211.42915028  9342.63536156  1697.89808596\n",
      "   454.75035704  7039.00092789 32268.1952539    717.75321846\n",
      " 10188.62617211  6486.55203107 33040.94402355  6446.48277341\n",
      "   380.25836256  3657.58938964 10399.75787128  6320.99779274\n",
      "   311.98441851  4000.6323829  18033.02380861  7270.23554208\n",
      "  7625.70660139 40048.84667968   243.6609398   4231.18182632\n",
      "  1087.39870834 13221.97624033  4341.74449212 39199.5053126\n",
      "  1060.24079076   382.7574531  58231.40945311  5755.71345713\n",
      "  7149.89056134 23768.67906254  6454.79600093  7631.03248034\n",
      "   711.34875373   281.9083657    246.27479285   907.47615474\n",
      "   274.04974743  4071.72062978  7813.59795889   344.70762242\n",
      "   346.58983573   231.50121284 46742.95535177  4344.55507806\n",
      "  9360.65812509  6375.38643079 11725.67926748  6974.8251563\n",
      "  9743.34582035   231.40424232 48640.24800779 46426.4855862\n",
      "   577.41385571  6889.77866214  8218.31504885  2213.92658806\n",
      "  9121.99751935   235.32125171  7961.4632764    326.81083514\n",
      "  6862.13894526   336.10097569   233.25490266   400.27132136\n",
      "   325.81103526 17469.39005878  8163.74219725 38428.42304675\n",
      "   587.17694325  2396.26014905  3847.51680409  3750.99011482\n",
      " 31860.96337906  7133.07290031   780.35920709 27688.98341812\n",
      "  3129.00754622   219.18749046  8834.34810539  1775.82619264\n",
      "  9862.20474616   438.58539761  9160.80804688   261.79316793\n",
      "   289.69132556   281.6979052   7123.22679183   426.51744474\n",
      " 63203.06378927 56024.46156269 11795.80846663   234.04383893\n",
      "  4138.75145773   285.10830674 10685.45679702  5076.8543163\n",
      "   240.10845946   691.49910383   222.65277326  8254.19262718\n",
      " 11582.72203121   229.78758042 47681.32445314   237.13067028\n",
      "   414.08441409   630.46608126  3922.65178459   274.23250434\n",
      "  8736.3349611  23436.1310939   7113.89888172   638.52558915\n",
      "   226.54935109 11183.25059564 35682.87070315  8696.20821294\n",
      "  3619.10390363   606.53538627   586.46388966   455.15907125\n",
      "   917.82692569 11304.18542969 23270.62935554   222.80751423\n",
      "  8497.73415033  8328.02040035   281.50890587  9547.51099618\n",
      "  3599.21885274 11758.74293938  6792.33778326   573.00212539\n",
      "  7250.70061034  1120.90951914  8204.11000477  3465.46879422\n",
      "  6882.62256359 58236.70597655 57284.47042997 43964.12250027\n",
      "  1775.7715015  10364.22934586   253.70858982  2664.56698749\n",
      "  1002.94809001   256.18525339   235.11940917 12216.56109382\n",
      "  8721.40487317 43124.4206642   8258.58597681   424.69901653\n",
      " 10457.90551772 61215.02140633  8250.6617189  56404.5033986\n",
      "  8794.16873054 40178.08929692  9133.85398424  1015.73866094\n",
      " 47102.59328152  5108.01466298  5925.28590322   311.16039712\n",
      " 31850.63285167   463.21023495   654.50242056   380.66520243\n",
      "  9193.40812495  7296.95369129   232.10236352   229.86021976\n",
      " 33531.16882829  6595.48500481   284.81397076   342.53954785\n",
      "  8229.09313961 47250.4963281   7875.00685049 57506.15418001\n",
      " 33860.95535178  8104.70025888   243.65256082 54797.05347667\n",
      "  9275.72969708   442.8848561    393.17187091  4159.28151372\n",
      " 55509.79378915 50039.82179696  6915.50985362 39325.94578139\n",
      "   222.69689284 10362.74785157  3585.80151846  8720.25345708\n",
      "  8904.43726562   311.4015218    362.4692365    421.48461744\n",
      " 15239.73874996   236.37374809  7986.69311525 35644.47074225\n",
      "  2517.32502706 42291.39043    64149.84589853 13973.86722646\n",
      "   357.65699823 59013.44839842   365.26304298   416.83278577\n",
      " 11418.33243164  4104.97891614 11700.95110329 39775.33617196\n",
      "  7537.38523916  2305.20685055  7135.33578108 36674.07996104\n",
      "   594.41545524   709.61371162 57203.89195338  3685.42607163\n",
      " 11702.74251947   251.0564128  19299.94476593   316.20768506\n",
      " 39252.23128919   225.72383874   331.88152342   237.25173002\n",
      " 54932.40812509 12077.08480468 11774.80090821  4334.73425286\n",
      " 11932.22156252  6731.5794042  47094.25203147 10166.99605492\n",
      " 35658.54382821 60978.58761732  9393.43861328 10215.96768553\n",
      "   433.96045957   705.69726394  6572.1308496  10138.77850612\n",
      " 10704.12851576 10418.42069336  3712.81045426  6414.15068826\n",
      " 29089.12611344 36873.15828138 10579.34170897  7204.93938946\n",
      " 10666.75210944 11305.94992192   572.21151426  4382.9415773\n",
      "   423.494341     451.04292372  6860.83244157   439.65839962\n",
      "  2302.72837388  6387.1540137    383.24323503  9309.35715839\n",
      "  7569.95556143  3683.40471211  3849.38210435  7261.74237313\n",
      "  1002.67420873  8207.79372069   367.35303781 12204.63488285\n",
      "  9324.58526395   437.58365358 56868.97785174   920.99329481\n",
      "  1572.39140753  6382.66402356  3549.71035156   422.15813566\n",
      "  9237.11489268   772.45483258 37214.08238282   240.96162082\n",
      "   455.29543068   387.7617673    264.46505988 13380.36740242\n",
      " 11486.70757813  7346.25410151   253.78538258  7910.96455568\n",
      "   276.45554745   237.32917977  6854.87039065  8936.30691427\n",
      "  6171.24708987  5988.17748052  9243.50743181  3883.38675792\n",
      "   736.91680533  1149.21449224  6494.89579586  7421.03685033\n",
      " 10278.92187487  4137.26057876  8275.06962912  7716.34230929\n",
      " 11673.38660153 19285.46892584  1004.69025736  3907.52527096\n",
      "   350.84184154  2603.17206309   660.8060694   7195.72005856\n",
      " 11743.88488288   435.91276674  6356.82374523 56116.66816431\n",
      "   745.33957156   244.575132     650.22531994 10349.46675776\n",
      " 59660.40417975  4602.35681171  6239.68859864   532.0564995\n",
      " 43619.03050795  9271.30695306   732.82178948  3850.85911605\n",
      "   327.58482598   290.4028251  32497.42091806 10678.81797875\n",
      " 11355.48777357   431.42708669 63105.07078144   224.54070274\n",
      " 40108.42871086  7411.35084944  7682.78484357   651.61074536\n",
      "   285.12766125  9868.0846974  10595.1556446    611.87484734\n",
      "   640.36635251   448.70040953 15430.19671873 10777.36210933\n",
      "  2720.65290039 38794.52375026 66088.50609388 11587.11408206\n",
      "   703.91697037 11227.91330081 18521.27080097 10341.14729488\n",
      "  8808.17209943   240.3166697   6590.88583492 11444.26493171\n",
      "   228.84383991  3332.59072511  1260.03489967 16501.74669938\n",
      " 58365.30316419  6970.95636216  7315.61319832]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Predict using the trained models\n",
    "# Use Linear Regression and Random Forest to predict the target values for the test set\n",
    "y_pred_lr = models[\"Linear Regression\"].predict(X_test)\n",
    "y_pred_rf = models[\"Random Forest Regressor\"].predict(X_test)\n",
    "\n",
    "# Show results\n",
    "print(\"✅ Predictions are ready!\")\n",
    "print(\"\\nLinear Regression predictions:\", y_pred_lr)\n",
    "print(\"\\nRandom Forest predictions:\", y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "c38c543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Performance:\n",
      "  R²   : 1.000\n",
      "  MAE  : 0\n",
      "  MSE  : 0\n",
      "  RMSE : 0\n",
      "\n",
      "Random Forest Regressor Performance:\n",
      "  R²   : 1.000\n",
      "  MAE  : 31\n",
      "  MSE  : 9434\n",
      "  RMSE : 97\n"
     ]
    }
   ],
   "source": [
    "# === Step 7: Metrics Function ===\n",
    "def evaluate_model(model_name, y_pred, y_true):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"  R²   : {r2:.3f}\")\n",
    "    print(f\"  MAE  : {mae:.0f}\")\n",
    "    print(f\"  MSE  : {mse:.0f}\")\n",
    "    print(f\"  RMSE : {rmse:.0f}\")\n",
    "\n",
    "evaluate_model(\"Linear Regression\", y_pred_lr, y_test)\n",
    "evaluate_model(\"Random Forest Regressor\", y_pred_rf, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "7c26daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single-row Sanity Check (Index 3) ===\n",
      "  Actual Close Price : 1133\n",
      "  LR Predicted       : 1133\n",
      "  RF Predicted       : 1140\n"
     ]
    }
   ],
   "source": [
    "# === Step 8: Single-row Sanity Check ===\n",
    "i = 3\n",
    "x_one_df = X_test.iloc[[i]]\n",
    "y_true = y_test.iloc[i]\n",
    "p_lr_one = float(models[\"Linear Regression\"].predict(x_one_df)[0])\n",
    "p_rf_one = float(models[\"Random Forest Regressor\"].predict(x_one_df)[0])\n",
    "print(\"\\n=== Single-row Sanity Check (Index 3) ===\")\n",
    "print(f\"  Actual Close Price : {y_true:.0f}\")\n",
    "print(f\"  LR Predicted       : {p_lr_one:.0f}\")\n",
    "print(f\"  RF Predicted       : {p_rf_one:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "4186b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Models saved successfully to the Models/ directory.\n"
     ]
    }
   ],
   "source": [
    "# === Step 9: Save  Models  === (New)\n",
    "joblib.dump(models[\"Linear Regression\"], \"./Models/bitcoin_lr_model.joblib\")\n",
    "joblib.dump(models[\"Random Forest Regressor\"], \"./Models/bitcoin_rf_model.joblib\")\n",
    "\n",
    "print(\"\\n✅ Models saved successfully to the Models/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "5119613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: scaler transform failed, using unscaled input. Error: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- Adj Close\n",
      "- Volume\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Day_Return\n",
      "- Volatility\n",
      "\n",
      "\n",
      "=== Custom Input Prediction ===\n",
      "Linear Regression: 2.787997226642443\n",
      "Random Forest    : 21653.515100409993\n"
     ]
    }
   ],
   "source": [
    "# === Step 10: Custom Input Prediction using helper ===\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "ART_DIR = \"./Models\"  # correct folder in this repo\n",
    "\n",
    "# artifact paths (use the filenames that exist in the repo)\n",
    "lr_path = os.path.join(ART_DIR, \"bitcoin_lr_model.joblib\")\n",
    "rf_path = os.path.join(ART_DIR, \"bitcoin_rf_model.joblib\")\n",
    "scaler_path = os.path.join(ART_DIR, \"bitcoin_scaler.pkl\")\n",
    "train_cols_path = os.path.join(ART_DIR, \"bitcoin_train_columns.json\")\n",
    "\n",
    "# Load models/artifacts with helpful errors\n",
    "if not os.path.exists(lr_path) or not os.path.exists(rf_path):\n",
    "    raise FileNotFoundError(f\"Expected model files not found in {ART_DIR}. \"\n",
    "                            f\"Found: {os.listdir(ART_DIR)}\")\n",
    "\n",
    "lr = joblib.load(lr_path)\n",
    "rf = joblib.load(rf_path)\n",
    "scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "\n",
    "if not os.path.exists(train_cols_path):\n",
    "    raise FileNotFoundError(f\"Training columns file not found: {train_cols_path}\")\n",
    "train_cols = json.load(open(train_cols_path))\n",
    "\n",
    "# Helper function to prepare features from raw input\n",
    "def prepare_features_from_raw(raw):\n",
    "    df = pd.DataFrame([raw])\n",
    "    # Example derived features (keep these if your model expects similar columns)\n",
    "    # Adjust names/logic to match the exact derived features you used during training\n",
    "    if \"High\" in df.columns and \"Low\" in df.columns:\n",
    "        df[\"High-Low\"] = df[\"High\"] - df[\"Low\"]\n",
    "    if \"Low\" in df.columns and \"Close\" in df.columns:\n",
    "        df[\"Low-Close\"] = df[\"Low\"] - df[\"Close\"]\n",
    "\n",
    "    # Example: day-of-week one-hot (adjust indices/naming to your training set)\n",
    "    for i in range(1, 8):  # 1..7 safer, adjust to your training encoding\n",
    "        df[f\"DayOfWeek_{i}\"] = 1 if i == raw.get(\"DayOfWeek\", 1) else 0\n",
    "\n",
    "    # Example: month one-hot — adapt months to what you used in training\n",
    "    months_used = list(range(1, 13))  # include all months by default; change if needed\n",
    "    for m in months_used:\n",
    "        df[f\"Month_{m}\"] = 1 if m == raw.get(\"Month\", 1) else 0\n",
    "\n",
    "    # Return dataframe of derived features (we'll align to train_cols later)\n",
    "    return df\n",
    "\n",
    "# Example raw custom input (change values as needed)\n",
    "custom = {\n",
    "    \"Open\": 45000, \"High\": 46000, \"Low\": 44000, \"Close\": 45500, \"Volume\": 35000,\n",
    "    \"DayOfWeek\": 1, \"Month\": 10\n",
    "}\n",
    "\n",
    "# Prepare features from raw input\n",
    "x_new = prepare_features_from_raw(custom)\n",
    "\n",
    "# Ensure all training columns exist and are in the correct order\n",
    "for c in train_cols:\n",
    "    if c not in x_new.columns:\n",
    "        # initialize missing columns to 0.0 (or NaN -> fill with 0.0)\n",
    "        x_new[c] = 0.0\n",
    "\n",
    "# Reorder columns exactly to training order\n",
    "x_new = x_new[train_cols].copy()\n",
    "\n",
    "# Apply scaler safely if available\n",
    "X_input = x_new.copy()\n",
    "if scaler is not None:\n",
    "    try:\n",
    "        # If scaler has mean_ attribute, use its length to decide which columns it was fit on\n",
    "        scaler_mean = getattr(scaler, \"mean_\", None)\n",
    "        if scaler_mean is not None and scaler_mean.shape[0] == len(train_cols):\n",
    "            X_input = pd.DataFrame(scaler.transform(X_input), columns=train_cols)\n",
    "        elif scaler_mean is not None and scaler_mean.shape[0] < len(train_cols):\n",
    "            # scaler was fit on a prefix of columns; apply to that prefix only\n",
    "            n = scaler_mean.shape[0]\n",
    "            prefix_cols = train_cols[:n]\n",
    "            X_input[prefix_cols] = scaler.transform(X_input[prefix_cols])\n",
    "        else:\n",
    "            # fallback - attempt to transform whole row (may raise if shapes mismatch)\n",
    "            X_input = pd.DataFrame(scaler.transform(X_input), columns=train_cols)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Warning: scaler transform failed, using unscaled input. Error:\", e)\n",
    "        X_input = x_new.copy()\n",
    "\n",
    "# Make predictions\n",
    "lr_pred = lr.predict(X_input)\n",
    "rf_pred = rf.predict(X_input)\n",
    "\n",
    "print(\"\\n=== Custom Input Prediction ===\")\n",
    "print(\"Linear Regression:\", float(lr_pred[0]))\n",
    "print(\"Random Forest    :\", float(rf_pred[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
